[["opdracht-3-gebruik-van-git-en-github.html", "Opdracht 3: Gebruik van Git en Github 3.1: CV 3.1: Een nieuwe vaardigheid", " Opdracht 3: Gebruik van Git en Github 3.1: CV Bij deze Portfolio opdracht moest er een CV gemaakt worden, deze is te vinden door op deze link naar de CV te klikken of door onderaan deze pagina te kijken. De code waarmee de CV gemaakt is, is te vinden door op deze link naar de Github repository te klikken. 3.1: Een nieuwe vaardigheid Een vooruit blik Het eerste gedeelte van deze opdracht gaat over waar ik mezelf zie over een aantal jaar op het geboed van lifesciences en bioinformatica. hiervoor heb ik eerst drie vragen voor mezelf moeten beantwoorden, namelijk; Waar wil ik over ~2 jaar zijn? Hoe sta ik er nu voor wat betreft dit doel? en Wat zou de volgende vaardigheid zijn om te leren? deze vragen heb ik hieronder beantwoord. Waar wil ik over ~2 jaar zijn? Over 2 jaar zou ik graag student af willen zijn en een vaste baan hebben in de life sciences. waar mijn voorkeur altijd uit zal blijven gaan naar op heb lab werken heb ik de afgelopen periode ook een passie gevonden voor data visualisatie. het leukste hieraan vind ik dat ik echt kan zien wat ik aan het doen ben en dat ik kan zien hoe nuttig dat wel niet kan zijn. Hoe sta ik er nu voor wat betreft dit doel? Na het afgelopen blok durf ik te zeggen dat ik al best goed ben in data visualiseren, het is voor mij logisch en de basis zit er in ieder geval al heel goed in, ik ben nu op het punt dat ik denk een stap verder te kunnen. Wat zou de volgende vaardigheid zijn om te leren? Als volgende vaardigheid lijkt het mij leuk om niet zelf de visualisaties te bekijken, maar dit door een machine te kunnen laten doen. Deze machine zou ik graag zelf willen kunnen bouwen. Mijn plan Wat ik besloten heb om te doen voor deze opdracht sluit heel goe aan bij de laatste vraag. Ik zou graag willen leren om een machine te maken die data kan analyseren. De machine die ik wilde bouwen zou een AI zijn gecodeert in R en die getraind is op een dataset met MRI hersenscans. Dit wilde ik gaan doen aan de hand van het volgende plan; 1) Verzamel de trainingsdata: Verzamel een dataset met afbeeldingen van tumoren en niet-tumoren. Zorg ervoor dat de afbeeldingen correct zijn gelabeld. 2) Data voorbereiden: Bereid de data voor op training door de afbeeldingen in te laden, te normaliseren en eventueel te verkleinen naar een uniform formaat. 3) Train het model: Gebruik de gelabelde trainingsdata om een AI-model te trainen voor het classificeren van tumoren. Je kunt hiervoor een diep neuraal netwerk zoals een Convolutional Neural Network (CNN) gebruiken. Train het model met behulp van een geschikte trainingsmethode, zoals backpropagation met stochastic gradient descent. 4) Model opslaan: Sla het getrainde model op in een geschikt formaat, bijvoorbeeld .h5 of .pb, zodat het later kan worden geladen en gebruikt voor voorspellingen. 5) Model evaluatie: Evalueer het getrainde model met behulp van een aparte validatieset. Bereken de nauwkeurigheid, precisie, recall en F1-score van het model om de prestaties te beoordelen. 6) Voorspellingen maken: Schrijf een functie die een afbeelding als input neemt, het opgeslagen model laadt en de afbeelding classificeert als tumor of niet-tumor. Zorg ervoor dat de afbeelding correct wordt voorbewerkt voordat deze wordt doorgegeven aan het model. 7) Test de voorspellingsfunctie: Test de voorspellingsfunctie met enkele nieuwe, ongeziene afbeeldingen om te controleren of het model correcte voorspellingen maakt. Uitvoering Stap 1: Stap 1 was erg makkelijk, ik vond al snel een dataset met MRI hersenscans die open was voor iedereen om te gebruiken. Ik heb de data verkregen van de kaggle pagina. Dit is een dataset met MRI scans van hersenen en Mapped images. een mapped image is een zwart wit foto waarin aangegeven is waar de tumor zit op de bijbehorende MRI scan. Stap 2: Stap 2 ging een stuk moeizamer dan stap 1. bij stap 2 ging ik de data normaliseren en tot het zelfde formaat brengen(zodat de je je geen zorgen hoeft te maken over het formaat van de input). Na ruim 20 uur eraan werken en veel hulp van ChatGPT was het dan toch gelukt om de code te schrijven voor de AI. Ook was dit het moment dat ik er achter kwam dat ik voor het programmeren van een AI beter in python had kunnen programmeren dan in R. Omdat ik er al zo veel uren in had zitten vond ik het alleen sonde om alles weg te gooien, dus heb ik een poging gedaan om verder de gaan in R. De code voor de AI is te vinden in mijn bijbehorende github repository. Stap 3: Na het maken van de code kon de AI getraind worden, dit is gedaan met trainingsdata. deze is te downloaden via de bovenstaande link naar kaggle. deze dataset heb ik een klein beetje aangepast om er ook een kleinere test dataset uit te krijgen. Helaas zijn het getrainde model en de data te grote bestanden om te kunnen uploaden naar github, maar als u mij contacteert op nonne.hodes@gmail.com dan stuur ik graag de aangepaste data en/of het model naar u op. Stap 4: Het getrainde model is opgeslagen naar mijn laptop. Stap 5: Bij stap 5 kwam ik er helaas achter dat ik toch echt de keuze had moeten maken om in python te programmeren in plaats van in R. Na al ruim 40 uur aan dit bestand te hebben gewerkt kwam ik er tijdens het testen achter dat hij helaas niet werkt. Net de testdataset leek het eerst goed te gaan, toen ik er een afbeelding in deed die geen tumor heeft gaf hij aan dat er geen tumorvorming is, helaas zei hij kort daarna dat een MRI waar wel een tumor op zichtbaar was ook geen tumorvorming heeft. hiermee zijn stap 6 en 7 ook niet gelukt. Conclusie In deze opdracht heb ik geprobeerd een AI-model te trainen om tumoren te classificeren op basis van afbeeldingen. Ik heb een dataset verzameld en voorbewerkt, en vervolgens een model getraind met behulp van deep learning technieken. Hoewel ik de nodige stappen heb genomen en de code zorgvuldig heb geïmplementeerd, heb ik helaas geen succesvol resultaat behaald. Het model kon de afbeeldingen niet nauwkeurig classificeren en de voorspellingen waren inconsistent. Discussie Er zijn verschillende mogelijke redenen waarom het AI-model niet correct werkte en de gewenste resultaten niet heeft opgeleverd. Ten eerste kan de kwaliteit van de dataset een rol spelen. Het is mogelijk dat de dataset niet voldoende representatief was, waardoor het model moeite had met het generaliseren naar nieuwe, ongeziene afbeeldingen. Daarnaast kan de complexiteit van het probleem ook een rol hebben gespeeld. Het classificeren van tumoren op basis van afbeeldingen kan een uitdagende taak zijn, vooral als er subtiele verschillen zijn tussen tumoren en niet-tumoren. Een andere factor die de prestaties van het model kan hebben beïnvloed, is de keuze van de modelarchitectuur en de trainingsparameters. Het kan zijn dat we een meer geavanceerde modelarchitectuur of een optimalisatie van de hyperparameters hadden moeten gebruiken om betere resultaten te behalen. Hoewel we in dit project niet het gewenste resultaat hebben behaald, blijft het interessant om de uitdaging aan te gaan en het in de toekomst nogmaals te proberen. Python heeft een breed scala aan bibliotheken en frameworks voor machine learning en deep learning, zoals TensorFlow en PyTorch, die geavanceerde modellen en technieken bieden voor beeldclassificatie. Door gebruik te maken van deze tools en meer geavanceerde technieken te verkennen, is er een grotere kans op succes bij het trainen van een nauwkeurig tumorclassificatiemodel. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
